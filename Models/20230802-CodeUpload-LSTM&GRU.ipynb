{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt # picture\n",
    "from math import sqrt\n",
    "import openpyxl\n",
    "\n",
    "## pandas data processing\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "## keras neural network\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Input, Model, optimizers\n",
    "import keras.backend as K\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.normalization import BatchNormalization, LayerNormalization\n",
    "from keras.models import *\n",
    "from keras.optimizers import adam_v2 # adam is not available\n",
    "\n",
    "## sklearn data processing and cross-validation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transport the shape from two dimension(timestep, features) into three dimension(timestep, features, windowsize)\n",
    "def shape_trans(data, input_window, output_window):\n",
    "    \n",
    "    \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Transport the shape of raw/sequential data (two dimension) into the shape matching to deep learning model (three dimension)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_window : The window length of DL input (X)\n",
    "    output_window : The window length of DL output (Y) - the RR simulation is '1', the multi-pred is '7'\n",
    "\n",
    "    Route-Params\n",
    "    ------------\n",
    "    X : DL input with meteorological and hydrological features\n",
    "    Y : DL output with future runoff\n",
    "    id_total : Index of Y\n",
    "    X_wet, Y_wet, id_wet : Wet period (determined by BPX method) X, Y, index of Y\n",
    "    X_dry, Y_dry, id_dry : Dry period (determined by BPX method) X, Y, index of Y\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x, y, X_wet, Y_wet, X_dry, Y_dry : Three demension cells\n",
    "    id_total, id_wet, id_dry : index of corresponding Y\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    X_wet, Y_wet, id_wet = [], [], []\n",
    "    X_dry, Y_dry, id_dry = [], [], []\n",
    "    X, Y, id_total = [], [], []\n",
    "    i = data.index[0]\n",
    "    \n",
    "    while i < data.index[-1]-input_window-output_window:\n",
    "        j = i - data.index[0]\n",
    "        if data['class'][i] == data['class'][i+input_window]:\n",
    "            if data['class'][i+input_window] == 1:\n",
    "                X_wet.append(np.array(data.iloc[j:j+input_window,2:]).T)\n",
    "                Y_wet.append(np.array(data.iloc[j+input_window:j+input_window+output_window,2]).T)\n",
    "                id_wet.append(data.index[j+input_window])\n",
    "            else:\n",
    "                X_dry.append(np.array(data.iloc[j:j+input_window,2:]).T)\n",
    "                Y_dry.append(np.array(data.iloc[j+input_window:j+input_window+output_window,2]).T)\n",
    "                id_dry.append(data.index[j+input_window])\n",
    "            i = i + 1\n",
    "        else:\n",
    "            i = i + 1\n",
    "            \n",
    "    for i in range (data.shape[0]-input_window-output_window):\n",
    "        X.append(np.array(data.iloc[i:i+input_window,2:]).T)\n",
    "        Y.append(np.array(data.iloc[i+input_window:i+input_window+output_window,2]).T)\n",
    "        id_total.append(data.index[i+input_window])\n",
    "                \n",
    "    X_wet = np.array(X_wet) # wet period x\n",
    "    Y_wet = np.array(Y_wet) # wet period y\n",
    "    X_dry = np.array(X_dry) # dry period x\n",
    "    Y_dry = np.array(Y_dry) # dry period y\n",
    "    x = np.array(X)\n",
    "    y = np.array(Y)\n",
    "    id_total = np.array(id_total)\n",
    "    id_dry = np.array(id_dry) # dry period index\n",
    "    id_wet = np.array(id_wet) # wet period index\n",
    "    \n",
    "    return x, y, X_wet, Y_wet, X_dry, Y_dry, id_total, id_wet, id_dry\n",
    "\n",
    "# combine the wet and dry result into entire result\n",
    "def wet_dry_comb(runoff_wet,runoff_dry,id_wet,id_dry):\n",
    "    \n",
    "    \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Recombine the wet and dry predicted runoff sub-sequences into long sequence\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    runoff_wet,runoff_dry : Prediction of wet/dry runoff sub-sequences\n",
    "    id_wet,id_dry : Index of runoff_wet,runoff_dry\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    box_sort : long predicted sequence\n",
    "    \n",
    "    \"\"\"\n",
    "    box_wet = pd.DataFrame(runoff_wet)\n",
    "    box_wet.index = id_wet\n",
    "    box_dry = pd.DataFrame(runoff_dry)\n",
    "    box_dry.index = id_dry\n",
    "    box = pd.concat([box_wet,box_dry],axis=0)\n",
    "    box_sort = box.sort_index()\n",
    "    \n",
    "    return box_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "def attention_3d_block(inputs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Attention layer in neural network (Keras)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    input_dim = int(inputs.shape[2])\n",
    "    TIME_STEPS = int(inputs.shape[1])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "# plot the loss\n",
    "def loss_plot(hist, name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Loss change among epochs plot\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.plot(hist.history['loss'],color='r')\n",
    "    plt.plot(hist.history['val_loss'],color='g')\n",
    "    plt.title(name)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'test_loss','train_acc', 'test_acc'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "# Combined loss function\n",
    "def CombLoss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Combination of NSE (L2) and relative error (L1)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    a = 0.65\n",
    "    up = K.mean(K.square(y_true - y_pred))\n",
    "    down = K.mean(K.square(K.mean(y_true) - y_true))\n",
    "    bias = K.mean(K.abs(y_true - y_pred)) / y_true\n",
    "    return a * (up / down) + (1 - a) * bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "def LSTMPretrain(data_train, data_test, layer1_cell = 256, layer2_cell = 256, layer3_cell = 256, lrr_pre = 0.0005, lrr_tune = 0.0005, droprate = 0.3, epochs_pre = 300, epochs_tune = 500):\n",
    "    \n",
    "    input_window = 7\n",
    "    output_window = 1 # multi-step prediction is '7'; RR simulation is '1'\n",
    "\n",
    "    x_train, y_train, x1_train, y1_train, x0_train, y0_train, id_t_train, id_1_train, id_0_train = shape_trans(data_train, input_window, output_window)\n",
    "    x_val, y_val, x1_val, y1_val, x0_val, y0_val, id_t_val, id_1_val, id_0_val = shape_trans(data_test, input_window, output_window)\n",
    "    \n",
    "    print('- -'*30)\n",
    "    outputsize = y_train.shape[1]\n",
    "    time_steps = x_train.shape[1]\n",
    "    input_vector = x_train.shape[2]\n",
    "\n",
    "    inputs=Input(shape=(x_train.shape[1], x_train.shape[2],))\n",
    "\n",
    "    dens = Dense(1024)(inputs)\n",
    "    # dens = attention_3d_block(dens)\n",
    "    layer1 = BatchNormalization(trainable=True)(dens)\n",
    "    attention_probs = Dense(1024, activation='softmax', name='attention_vec')(layer1)\n",
    "    layer1 =  Multiply()([layer1, attention_probs])\n",
    "    layer1 = LSTM(units=layer1_cell, input_shape=(time_steps, input_vector),return_sequences=True)(layer1)\n",
    "    layer1 = BatchNormalization(trainable=True)(layer1)\n",
    "    layer2 = LSTM(units=layer2_cell, return_sequences=True)(layer1)\n",
    "    layer3 = LSTM(units=layer3_cell)(layer2)\n",
    "    drop = SpatialDropout1D(droprate)(layer2)\n",
    "    layer_flatten = Flatten()(drop)\n",
    "    layer_dense = Dense(32)(layer_flatten)\n",
    "    outputs = Dense(outputsize)(layer_dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    adam = adam_v2.Adam(learning_rate = lrr_pre)\n",
    "    model.compile(loss=CombLoss, optimizer=adam)\n",
    "#     model.summary()\n",
    "\n",
    "    # training\n",
    "    history = model.fit(x_train,y_train,batch_size=2048,epochs=epochs_pre,verbose=0,validation_data=(x_val,y_val))  \n",
    "    loss_plot(history,name='Pre-training Loss')\n",
    "    \n",
    "    pred = model.predict(x_val)\n",
    "    pred_1 = model.predict(x_wet_val)    \n",
    "    pred_0 = model.predict(x_dry_val)\n",
    "    modelname = 'runoff_pretrain_model.h5'\n",
    "    \n",
    "    model.save('modelbox/'+modelname)\n",
    "    \n",
    "    ## fine-tune_WET\n",
    "    for layer in model.layers [:-7]:\n",
    "        layer.trainable = False # freeze\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    adam = adam_v2.Adam(learning_rate = lrr_tune)\n",
    "    model.compile(loss =  CombLoss, optimizer = adam, metrics = CombLoss)\n",
    "\n",
    "    history = model.fit(x_wet_train,y_wet_train,batch_size=2048,epochs=epochs_tune,verbose=0,validation_data=(x_wet_val,y_wet_val))  \n",
    "    loss_plot(history,name='Fine-tuning Loss')\n",
    "    \n",
    "    pred_wet = model.predict(x_wet_val) \n",
    "    \n",
    "    model = load_model('modelbox/'+modelname,custom_objects={'CombLoss':CombLoss})\n",
    "    ## fine-tune_DRY\n",
    "    for layer in model.layers [:-7]:\n",
    "        layer.trainable = False # freeze\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    adam = adam_v2.Adam(learning_rate = lrr_tune)\n",
    "    model.compile(loss =  CombLoss, optimizer = adam, metrics = CombLoss)\n",
    "\n",
    "    history = model.fit(x_dry_train,y_dry_train,batch_size=2048,epochs=epochs_tune,verbose=0,validation_data=(x_dry_val,y_dry_val))  \n",
    "    loss_plot(history,name='Fine-tuning Loss')\n",
    "    \n",
    "    pred_dry = model.predict(x_dry_val)\n",
    "    \n",
    "    box_pred_tuned = wet_dry_comb(pred_wet,pred_dry,id_wet_val,id_dry_val)\n",
    "    box_pred_pre = wet_dry_comb(pred_1,pred_0,id_wet_val,id_dry_val)\n",
    "    box_act = wet_dry_comb(y_wet_val,y_dry_val,id_wet_val,id_dry_val)\n",
    "    \n",
    "        \n",
    "    return box_pred_tuned, box_pred_pre, box_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 4)\n",
    "for train_index, test_index in tscv.split(data):\n",
    "    data_train, data_test = data.iloc[train_index,:], data.iloc[test_index,:]\n",
    "    box_pred_tuned, box_pred_pre, box_act = LSTMPretrain(data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "def GRUPretrain(data_train, data_test, layer1_cell = 256, layer2_cell = 256, layer3_cell = 256, lrr_pre = 0.0005, lrr_tune = 0.0005, droprate = 0.3, epochs_pre = 300, epochs_tune = 500):\n",
    "    \n",
    "    input_window = 7\n",
    "    output_window = 1 # multi-step prediction is '7'; RR simulation is '1'\n",
    "\n",
    "    x_train, y_train, x1_train, y1_train, x0_train, y0_train, id_t_train, id_1_train, id_0_train = shape_trans(data_train, input_window, output_window)\n",
    "    x_val, y_val, x1_val, y1_val, x0_val, y0_val, id_t_val, id_1_val, id_0_val = shape_trans(data_test, input_window, output_window)\n",
    "    \n",
    "    print('- -'*30)\n",
    "    outputsize = y_train.shape[1]\n",
    "    time_steps = x_train.shape[1]\n",
    "    input_vector = x_train.shape[2]\n",
    "\n",
    "    inputs=Input(shape=(x_train.shape[1], x_train.shape[2],))\n",
    "\n",
    "    dens = Dense(1024)(inputs)\n",
    "    # dens = attention_3d_block(dens)\n",
    "    layer1 = BatchNormalization(trainable=True)(dens)\n",
    "    attention_probs = Dense(1024, activation='softmax', name='attention_vec')(layer1)\n",
    "    layer1 =  Multiply()([layer1, attention_probs])\n",
    "    layer1 = GRU(units=layer1_cell, input_shape=(time_steps, input_vector),return_sequences=True)(layer1)\n",
    "    layer1 = BatchNormalization(trainable=True)(layer1)\n",
    "    layer2 = GRU(units=layer2_cell, return_sequences=True)(layer1)\n",
    "    layer3 = GRU(units=layer3_cell)(layer2)\n",
    "    drop = SpatialDropout1D(droprate)(layer2)\n",
    "    layer_flatten = Flatten()(drop)\n",
    "    layer_dense = Dense(32)(layer_flatten)\n",
    "    outputs = Dense(outputsize)(layer_dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    adam = adam_v2.Adam(learning_rate = lrr_pre)\n",
    "    model.compile(loss=CombLoss, optimizer=adam)\n",
    "#     model.summary()\n",
    "\n",
    "    # training\n",
    "    history = model.fit(x_train,y_train,batch_size=2048,epochs=epochs_pre,verbose=0,validation_data=(x_val,y_val))  \n",
    "    loss_plot(history,name='Pre-training Loss')\n",
    "    \n",
    "    pred = model.predict(x_val)\n",
    "    pred_1 = model.predict(x_wet_val)    \n",
    "    pred_0 = model.predict(x_dry_val)\n",
    "    modelname = 'runoff_pretrain_model.h5'\n",
    "    \n",
    "    model.save('modelbox/'+modelname)\n",
    "    \n",
    "    ## fine-tune_WET\n",
    "    for layer in model.layers [:-7]:\n",
    "        layer.trainable = False # freeze\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    adam = adam_v2.Adam(learning_rate = lrr_tune)\n",
    "    model.compile(loss =  CombLoss, optimizer = adam, metrics = CombLoss)\n",
    "\n",
    "    history = model.fit(x_wet_train,y_wet_train,batch_size=2048,epochs=epochs_tune,verbose=0,validation_data=(x_wet_val,y_wet_val))  \n",
    "    loss_plot(history,name='Fine-tuning Loss')\n",
    "    \n",
    "    pred_wet = model.predict(x_wet_val) \n",
    "    \n",
    "    model = load_model('modelbox/'+modelname,custom_objects={'CombLoss':CombLoss})\n",
    "    ## fine-tune_DRY\n",
    "    for layer in model.layers [:-7]:\n",
    "        layer.trainable = False # freeze\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    adam = adam_v2.Adam(learning_rate = lrr_tune)\n",
    "    model.compile(loss =  CombLoss, optimizer = adam, metrics = CombLoss)\n",
    "\n",
    "    history = model.fit(x_dry_train,y_dry_train,batch_size=2048,epochs=epochs_tune,verbose=0,validation_data=(x_dry_val,y_dry_val))  \n",
    "    loss_plot(history,name='Fine-tuning Loss')\n",
    "    \n",
    "    pred_dry = model.predict(x_dry_val)\n",
    "    \n",
    "    box_pred_tuned = wet_dry_comb(pred_wet,pred_dry,id_wet_val,id_dry_val)\n",
    "    box_pred_pre = wet_dry_comb(pred_1,pred_0,id_wet_val,id_dry_val)\n",
    "    box_act = wet_dry_comb(y_wet_val,y_dry_val,id_wet_val,id_dry_val)\n",
    "    \n",
    "        \n",
    "    return box_pred_tuned, box_pred_pre, box_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits = 4)\n",
    "for train_index, test_index in tscv.split(data):\n",
    "    data_train, data_test = data.iloc[train_index,:], data.iloc[test_index,:]\n",
    "    box_pred_tuned, box_pred_pre, box_act = GRUPretrain(data_train, data_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-GPU]",
   "language": "python",
   "name": "conda-env-tensorflow-GPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
